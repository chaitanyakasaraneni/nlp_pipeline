{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "\n",
    "This notebook contains an example of text processing stage of NLP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/kc.kasaraneni/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kc.kasaraneni/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kc.kasaraneni/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/kc.kasaraneni/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove emails\n",
    "    text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', text, flags=re.MULTILINE)\n",
    "    \n",
    "    print('\\nCleaning output:\\n')\n",
    "    print(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normailze_text(text):\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra characters\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n",
    "    \n",
    "    # Remove punctuation characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text) \n",
    "    \n",
    "    # Remove symbols\n",
    "    text = re.sub(r'[^A-Za-z\\s]',r'',text)\n",
    "    text = re.sub(r'\\n',r'',text)\n",
    "    \n",
    "    print('\\nNormalization output:\\n')\n",
    "    print(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \n",
    "    #Tokenize words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    print('\\nTokenization output:\\n')\n",
    "    print(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    token_list = []\n",
    "    \n",
    "    for word in tokens:\n",
    "        if not word in stop_words:\n",
    "            token_list.append(word)\n",
    "            \n",
    "    print('\\nOutput after removing stop words:\\n')\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parts of speech tagging & Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_ner(tokens):\n",
    "    \n",
    "    #POS tagging\n",
    "    pos = pos_tag(tokens)\n",
    "    \n",
    "    print('\\nParts of Speech Tagging:\\n')\n",
    "    print(pos)\n",
    "    \n",
    "    #NER\n",
    "    ner = ne_chunk(pos)\n",
    "    \n",
    "    print('\\nNamed Entity Recognition:\\n')\n",
    "    print(ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stemming and Lemmatizing Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lem_words(tokens):\n",
    "    \n",
    "    # Stemming tokens\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    print('\\nStemming Output:\\n')\n",
    "    print(tokens)\n",
    "    \n",
    "    #Lemmatizing tokens\n",
    "    tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "\n",
    "    print('\\nLemmatizing Output:\\n')\n",
    "    print(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(text):\n",
    "    \n",
    "    # Clean Text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Normalize Text\n",
    "    text = normailze_text(text)\n",
    "    \n",
    "    # Tokenize Text\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    # Remove Stop words\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Display POS & NER\n",
    "    pos_ner(tokens)\n",
    "    \n",
    "    # Stem & Lemmatize Tokens\n",
    "    tokens = stem_lem_words(tokens)\n",
    "\n",
    "    print('\\nThe input text after processing:\\n')\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning output:\n",
      "\n",
      "The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ? Learn more at \n",
      "\n",
      "Normalization output:\n",
      "\n",
      "the first time you see the second renaissance it may look boring  look at it at least twice and definitely watch part   it will change your view of the matrix  are the human people the ones who started the war   is ai a bad thing   learn more at \n",
      "\n",
      "Tokenization output:\n",
      "\n",
      "['the', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definitely', 'watch', 'part', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human', 'people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing', 'learn', 'more', 'at']\n",
      "\n",
      "Output after removing stop words:\n",
      "\n",
      "\n",
      "Parts of Speech Tagging:\n",
      "\n",
      "[('first', 'JJ'), ('time', 'NN'), ('see', 'VB'), ('second', 'JJ'), ('renaissance', 'NN'), ('may', 'MD'), ('look', 'VB'), ('boring', 'VBG'), ('look', 'NN'), ('least', 'JJS'), ('twice', 'RB'), ('definitely', 'RB'), ('watch', 'JJ'), ('part', 'NN'), ('change', 'NN'), ('view', 'NN'), ('matrix', 'JJ'), ('human', 'JJ'), ('people', 'NNS'), ('ones', 'NNS'), ('started', 'VBD'), ('war', 'NN'), ('ai', 'NN'), ('bad', 'JJ'), ('thing', 'NN'), ('learn', 'NN')]\n",
      "\n",
      "Named Entity Recognition:\n",
      "\n",
      "(S\n",
      "  first/JJ\n",
      "  time/NN\n",
      "  see/VB\n",
      "  second/JJ\n",
      "  renaissance/NN\n",
      "  may/MD\n",
      "  look/VB\n",
      "  boring/VBG\n",
      "  look/NN\n",
      "  least/JJS\n",
      "  twice/RB\n",
      "  definitely/RB\n",
      "  watch/JJ\n",
      "  part/NN\n",
      "  change/NN\n",
      "  view/NN\n",
      "  matrix/JJ\n",
      "  human/JJ\n",
      "  people/NNS\n",
      "  ones/NNS\n",
      "  started/VBD\n",
      "  war/NN\n",
      "  ai/NN\n",
      "  bad/JJ\n",
      "  thing/NN\n",
      "  learn/NN)\n",
      "\n",
      "Stemming Output:\n",
      "\n",
      "['first', 'time', 'see', 'second', 'renaiss', 'may', 'look', 'bore', 'look', 'least', 'twice', 'definit', 'watch', 'part', 'chang', 'view', 'matrix', 'human', 'peopl', 'one', 'start', 'war', 'ai', 'bad', 'thing', 'learn']\n",
      "\n",
      "Lemmatizing Output:\n",
      "\n",
      "['first', 'time', 'see', 'second', 'renaiss', 'may', 'look', 'bore', 'look', 'least', 'twice', 'definit', 'watch', 'part', 'chang', 'view', 'matrix', 'human', 'peopl', 'one', 'start', 'war', 'ai', 'bad', 'thing', 'learn']\n",
      "\n",
      "The input text after processing:\n",
      "\n",
      "['first', 'time', 'see', 'second', 'renaiss', 'may', 'look', 'bore', 'look', 'least', 'twice', 'definit', 'watch', 'part', 'chang', 'view', 'matrix', 'human', 'peopl', 'one', 'start', 'war', 'ai', 'bad', 'thing', 'learn']\n"
     ]
    }
   ],
   "source": [
    "inp_text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and definitely watch part 2. It will change your view of the matrix. Are the human people the ones who started the war ? Is AI a bad thing ? Learn more at https://www.ai.com/test\"\n",
    "\n",
    "print(main(inp_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
